# =============================================================================
# WaveGo Agent Configuration
# =============================================================================
# This file contains all hardware and software configuration parameters.
# Modify these values according to your setup.

# -----------------------------------------------------------------------------
# Hardware Configuration
# -----------------------------------------------------------------------------
hardware:
  serial:
    port: "/dev/ttyS0"
    baudrate: 115200
    timeout: 1.0

  camera:
    device_id: 0
    width: 640
    height: 480
    fps: 30

  audio:
    # Microphone settings
    mic_device_index: null  # null for default device
    sample_rate: 16000
    chunk_size: 1024
    record_seconds: 5  # Max recording time for voice input
    
    # Speaker settings
    speaker_device_index: null  # null for default device

# -----------------------------------------------------------------------------
# LLM API Configuration
# -----------------------------------------------------------------------------
llm:
  provider: "openai"  # Options: "openai", "azure", "local"
  model: "gpt-4o-mini"  # Recommended for cost/performance balance
  
  # API Key - Choose ONE of these methods:
  # Method 1: Direct key (easier, less secure)
  # api_key: "sk-your-api-key-here"
  
  # Method 2: Environment variable (recommended for security)
  api_key_env: "API_KEY"  # Reads from this env variable
  
  # Optional: Custom API endpoint (for Azure, local LLM, etc.)
  # base_url: "https://your-endpoint.openai.azure.com/v1"
  
  max_tokens: 500
  temperature: 0.7
  
  # System prompt for the robot assistant
  system_prompt: |
    You are an intelligent assistant controlling a quadruped robot dog called WaveGo.
    You can help the user by executing movement commands and answering questions.
    
    For SIMPLE commands, respond with:
    {
      "intent": "move|look|stop|query|gesture|light|unknown",
      "action": "specific action",
      "parameters": {"direction": "forward", "duration": 1.0, "color": "blue"},
      "reply": "Natural language response"
    }
    
    For COMPLEX/SEQUENTIAL commands (like "先前进再后退", "walk forward then turn"), respond with:
    {
      "intent": "sequence",
      "actions": [
        {"action": "forward", "duration": 2.0},
        {"action": "backward", "duration": 2.0}
      ],
      "reply": "Description of what I'll do"
    }
    
    Available actions for sequences:
    - Movement: forward, backward, left, right, stop
    - Head: look_up, look_down, look_left, look_right
    - Gestures: jump, handshake, steady
    - Lights: light_blue, light_red, light_green, light_yellow, light_cyan, light_magenta, light_cyber, light_off
    - Control: wait (pause), buzzer_on, buzzer_off
    
    Consider vision_state when making decisions:
    - If obstacle_ahead is true, do NOT move forward
    
    Always respond in valid JSON format.

# -----------------------------------------------------------------------------
# Speech Recognition Configuration
# -----------------------------------------------------------------------------
speech:
  # STT Engine: "google", "whisper_api", "whisper_local", "vosk"
  stt_engine: "google"
  
  # Wake word (optional, set to null to disable)
  wake_word: null  # e.g., "hey robot"
  
  # Language
  language: "en-US"
  
  # Whisper settings (if using whisper)
  whisper:
    model: "base"  # tiny, base, small, medium, large
    api_key_env: "OPENAI_API_KEY"

# -----------------------------------------------------------------------------
# Text-to-Speech Configuration  
# -----------------------------------------------------------------------------
tts:
  # TTS Engine: "pyttsx3", "gtts", "openai"
  # - pyttsx3: Offline, requires espeak (sudo apt install espeak espeak-ng)
  # - gtts: Google TTS, requires internet, better quality
  # - openai: OpenAI TTS API, highest quality, requires API key
  engine: "gtts"  # Changed to gtts as default (more reliable)
  
  # Voice settings (for pyttsx3)
  voice_rate: 150  # Words per minute
  voice_volume: 0.9  # 0.0 to 1.0
  
  # OpenAI TTS settings (if using)
  openai_tts:
    model: "tts-1"
    voice: "alloy"  # alloy, echo, fable, onyx, nova, shimmer

# -----------------------------------------------------------------------------
# Vision Processing Configuration
# -----------------------------------------------------------------------------
vision:
  # Enable/disable vision processing
  enabled: true
  
  # Obstacle detection
  obstacle_detection:
    enabled: true
    min_area: 5000  # Minimum contour area to consider as obstacle
    roi_top: 0.3    # Region of interest (top 30% to 70% of frame)
    roi_bottom: 0.7
  
  # Color detection (HSV ranges)
  color_detection:
    enabled: true
    default_color:
      h_center: 34  # Yellow-ish
      h_range: 15
      s_min: 100
      s_max: 255
      v_min: 100
      v_max: 255
  
  # Face detection
  face_detection:
    enabled: true
    scale_factor: 1.2
    min_neighbors: 5
    min_size: [20, 20]
  
  # Motion detection
  motion_detection:
    enabled: true
    min_area: 2000
    threshold: 25

# -----------------------------------------------------------------------------
# Agent Behavior Configuration
# -----------------------------------------------------------------------------
agent:
  # Safety constraints
  safety:
    obstacle_stop: true  # Stop if obstacle detected ahead
    max_continuous_move_time: 5.0  # Max seconds for continuous movement
    
  # Movement parameters
  movement:
    default_speed: 100
    default_duration: 1.0  # Default movement duration in seconds
    
  # Response behavior
  response:
    speak_confirmation: true  # Speak before executing action
    speak_completion: false   # Speak after action complete

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  file: "wavego_agent.log"
  console: true
